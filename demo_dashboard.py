# --- START OF FILE demo_dashboard.py ---
import pandas as pd
import os
import sys
from datetime import datetime, timedelta
import random

# --- Configuraci√≥n para Colab y rutas ---
# Ajusta esta ruta a la ra√≠z de tu proyecto en Drive
project_root = '/home/master/workspace/rag_sandbox_evaluation'

# A√±adir la ra√≠z del proyecto y src al sys.path para importaciones
if project_root not in sys.path:
    sys.path.insert(0, project_root)
src_path = os.path.join(project_root, 'src')
if src_path not in sys.path:
    sys.path.insert(0, src_path)

print(f"sys.path actualizado: {sys.path[:3]}...")

# Importa la funci√≥n del dashboard desde tu nuevo m√≥dulo
try:
    from src.demo_dashboard_rag import crear_dashboard_evaluacion
    print("‚úÖ Funci√≥n 'crear_dashboard_evaluacion' importada con √©xito desde 'demo_dashboard_rag'.")
except ImportError as e:
    print(f"‚ùå Error al importar 'crear_dashboard_evaluacion': {e}")
    print("Aseg√∫rate de que 'src/demo_dashboard_rag.py' existe y '__init__.py' est√° en 'src/'.")
    sys.exit(1) # Salir si no podemos importar la funci√≥n

# --- 1. Generar Datos de Prueba (Simulando results_df con m√°s dimensiones) ---
print("\n‚öôÔ∏è Generando datos de prueba para la demo del dashboard con datos temporales y categor√≠as...")

# Definimos un conjunto de preguntas comunes y sus categor√≠as
questions_info = [
    {"question": "¬øQui√©n cre√≥ Python?", "category": "Tecnolog√≠a"},
    {"question": "¬øQu√© necesita una planta para la fotos√≠ntesis?", "category": "Ciencia"},
    {"question": "¬øQu√© es un Sprint en Agile?", "category": "Gesti√≥n de Proyectos"},
    {"question": "¬øQu√© es un LLM?", "category": "Tecnolog√≠a"},
    {"question": "¬øCu√°l es la capital de Francia?", "category": "Geograf√≠a"},
    {"question": "¬øQui√©n escribi√≥ 'Cien a√±os de soledad'?", "category": "Literatura"},
    {"question": "¬øQu√© base de datos vectorial es escalable?", "category": "Tecnolog√≠a"},
    {"question": "¬øEs Python un lenguaje compilado?", "category": "Tecnolog√≠a"},
]

# Definimos las fechas de evaluaci√≥n
evaluation_dates = [
    datetime(2025, 8, 1),
    datetime(2025, 9, 1),
    datetime(2025, 9, 20) # A√±adir una tercera fecha para ver m√°s evoluci√≥n
]

all_demo_data = []

# Ground truths y contextos de referencia (simplificados para demo)
# En un escenario real, esto se recuperar√≠a de alg√∫n lugar
reference_data = {
    "¬øQui√©n cre√≥ Python?": {
        "ground_truth": "Python fue creado por Guido van Rossum en 1991.",
        "contexts": ["Python es un lenguaje de programaci√≥n interpretado, de alto nivel y de prop√≥sito general. Creado por Guido van Rossum y lanzado por primera vez en 1991."]
    },
    "¬øQu√© necesita una planta para la fotos√≠ntesis?": {
        "ground_truth": "Las plantas usan luz solar, agua y di√≥xido de carbono para la fotos√≠ntesis.",
        "contexts": ["La fotos√≠ntesis es el proceso mediante el cual las plantas usan la luz solar, el agua y el di√≥xido de carbono para crear su propio alimento."]
    },
    "¬øQu√© es un Sprint en Agile?": {
        "ground_truth": "Un Sprint es una iteraci√≥n corta en el desarrollo √°gil de software.",
        "contexts": ["El desarrollo √°gil de software se basa en iteraciones cortas llamadas Sprints. Scrum es un marco popular para implementar Agile."]
    },
    "¬øQu√© es un LLM?": {
        "ground_truth": "Un Large Language Model (LLM) es un modelo de lenguaje con muchos par√°metros.",
        "contexts": ["Un Large Language Model (LLM) es un modelo de lenguaje con muchos par√°metros, capaz de entender y generar texto similar al humano."]
    },
    "¬øCu√°l es la capital de Francia?": {
        "ground_truth": "La capital de Francia es Par√≠s.",
        "contexts": ["La capital de Francia es Par√≠s."]
    },
    "¬øQui√©n escribi√≥ 'Cien a√±os de soledad'?": {
        "ground_truth": "Gabriel Garc√≠a M√°rquez fue el autor de 'Cien a√±os de soledad'.",
        "contexts": ["'Cien a√±os de soledad' es una novela del escritor colombiano Gabriel Garc√≠a M√°rquez."]
    },
    "¬øQu√© base de datos vectorial es escalable?": {
        "ground_truth": "Milvus es una base de datos vectorial de c√≥digo abierto altamente escalable.",
        "contexts": ["Milvus es una base de datos vectorial de c√≥digo abierto altamente escalable, dise√±ada para gestionar embeddings de aprendizaje autom√°tico."]
    },
    "¬øEs Python un lenguaje compilado?": {
        "ground_truth": "Python es un lenguaje interpretado.",
        "contexts": ["Python es un lenguaje de programaci√≥n interpretado, de alto nivel y de prop√≥sito general."]
    },
}


# Generar datos para cada modelo, pregunta y fecha
for eval_date in evaluation_dates:
    for model_name in ['Modelo_A', 'Modelo_B', 'Modelo_C_Experimental']: # A√±adir un tercer modelo para m√°s complejidad
        for q_info in questions_info:
            question = q_info['question']
            category = q_info['category']
            ref_data = reference_data.get(question, {"ground_truth": "N/A", "contexts": ["N/A"]})

            # Simular respuestas y m√©tricas con variaciones plausibles
            # y que mejoren/empeoren ligeramente con el tiempo o entre modelos
            base_faithfulness = 0.85
            base_relevancy = 0.85
            base_precision = 0.85
            base_recall = 0.85
            base_latency = 1.0
            base_tokens = 60
            base_satisfaction = 1 # assume good initially

            # Introduce variaciones basadas en el modelo y la fecha
            if model_name == 'Modelo_A':
                # Modelo A es bastante estable
                faith = base_faithfulness + random.uniform(-0.05, 0.05)
                relev = base_relevancy + random.uniform(-0.05, 0.05)
                prec = base_precision + random.uniform(-0.05, 0.05)
                rec = base_recall + random.uniform(-0.05, 0.05)
                lat = base_latency + random.uniform(-0.2, 0.2)
                tok = base_tokens + random.randint(-10, 10)
            elif model_name == 'Modelo_B':
                # Modelo B mejora con el tiempo, pero es m√°s lento
                # Mejoras simuladas para fechas posteriores
                date_factor = (eval_date - evaluation_dates[0]).days / 30 # ~0, 1, 2
                faith = (base_faithfulness - 0.1) + date_factor * 0.05 + random.uniform(-0.03, 0.03)
                relev = (base_relevancy - 0.05) + date_factor * 0.03 + random.uniform(-0.03, 0.03)
                prec = (base_precision - 0.05) + date_factor * 0.02 + random.uniform(-0.03, 0.03)
                rec = (base_recall - 0.05) + date_factor * 0.02 + random.uniform(-0.03, 0.03)
                lat = (base_latency + 0.3) + date_factor * -0.05 + random.uniform(-0.1, 0.1) # M√°s lento, pero mejora un poco
                tok = (base_tokens + 15) + random.randint(-5, 5) # M√°s tokens
            else: # Modelo_C_Experimental - m√°s vol√°til, puede ser muy bueno o muy malo
                date_factor = (eval_date - evaluation_dates[0]).days / 30
                faith = (base_faithfulness + random.uniform(-0.2, 0.1)) + date_factor * 0.01
                relev = (base_relevancy + random.uniform(-0.2, 0.1)) + date_factor * 0.01
                prec = (base_precision + random.uniform(-0.2, 0.1)) + date_factor * 0.01
                rec = (base_recall + random.uniform(-0.2, 0.1)) + date_factor * 0.01
                lat = (base_latency + random.uniform(-0.5, 0.5)) + date_factor * -0.01
                tok = (base_tokens + random.randint(-20, 20))

            # Asegurar que las m√©tricas de 0 a 1 est√©n en ese rango
            faith = max(0.0, min(1.0, faith))
            relev = max(0.0, min(1.0, relev))
            prec = max(0.0, min(1.0, prec))
            rec = max(0.0, min(1.0, rec))

            # Simular la satisfacci√≥n basada en faithfulness y relevancy
            satis = 1 if (faith > 0.75 and relev > 0.75) else 0

            # Simular respuestas variadas
            simulated_answer = f"Respuesta simulada de {model_name} para '{question}' en {eval_date.strftime('%Y-%m-%d')}."


            all_demo_data.append({
                'timestamp': eval_date,
                'model_name': model_name,
                'question_category': category,
                'question': question,
                'answer': simulated_answer,
                'ground_truth': ref_data['ground_truth'],
                'contexts': ref_data['contexts'],
                'faithfulness': faith,
                'answer_relevancy': relev,
                'context_precision': prec,
                'context_recall': rec,
                'latency': lat,
                'answer_tokens': tok,
                'simulated_satisfaction': satis
            })

demo_results_df = pd.DataFrame(all_demo_data)

# Convertir timestamp a formato de fecha para visualizaci√≥n si es necesario
demo_results_df['timestamp'] = pd.to_datetime(demo_results_df['timestamp'])


# --- 2. Preparar Directorio de Salida ---
output_dir = os.path.join(project_root, 'output')
os.makedirs(output_dir, exist_ok=True)
output_filename = os.path.join(output_dir, "rag_advanced_dashboard_demo.html")

# --- 3. Generar el Dashboard ---
print(f"üöÄ Generando dashboard de demostraci√≥n en: {output_filename}")

metric_descriptions = {
    'faithfulness': "Mide la precisi√≥n y la consistencia de la respuesta del modelo **RAG** con respecto a la informaci√≥n proporcionada en los documentos fuente. Una puntuaci√≥n alta indica que la respuesta no contiene alucinaciones y se basa directamente en el contexto recuperado.",
    'answer_relevancy': "Eval√∫a qu√© tan directa y completamente responde la respuesta generada por el **modelo RAG** a la pregunta del usuario. Ignora la veracidad de la respuesta.",
    'context_precision': "Indica qu√© tan relevante es el contexto recuperado para responder a la pregunta. Una puntuaci√≥n alta significa que los pasajes recuperados son directamente √∫tiles.",
    'context_recall': "Mide la exhaustividad del contexto recuperado, es decir, si todos los hechos necesarios para responder la pregunta est√°n presentes en el contexto.",
    'latency': "Tiempo en segundos que tarda el modelo **RAG** en generar una respuesta.",
    'answer_tokens': "N√∫mero de tokens en la respuesta generada por el **modelo RAG**.",
    'simulated_satisfaction': "M√©trica binaria (0/1) que indica la satisfacci√≥n simulada del usuario con la respuesta del **modelo RAG**. Puede representar una evaluaci√≥n humana simplificada."
}

crear_dashboard_evaluacion(
    df=demo_results_df,
    output_path=output_filename,
    dashboard_title="Evaluaci√≥n Comparativa de Sistemas RAG (Temporal y por Categor√≠a)", # T√≠tulo m√°s expl√≠cito
    metric_descriptions=metric_descriptions,
    models_to_compare=demo_results_df['model_name'].unique().tolist()
)

print("\n‚úÖ Demo del dashboard generada con √©xito.")
print(f"Abre el archivo '{output_filename}' en tu navegador web para verlo.")
# --- END OF FILE demo_dashboard.py ---