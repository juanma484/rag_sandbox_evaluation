# ğŸ“Š Framework de EvaluaciÃ³n de Sistemas RAG (Retrieval Augmented Generation)

Este proyecto es un framework extensible y automatizado diseÃ±ado para evaluar la calidad y el rendimiento de cualquier sistema RAG (Retrieval Augmented Generation). Proporciona herramientas para generar datasets de evaluaciÃ³n, ejecutar pruebas contra un RAG externo y visualizar los resultados a travÃ©s de un dashboard interactivo.

## âœ¨ CaracterÃ­sticas Principales

-   **EvaluaciÃ³n Flexible:** DiseÃ±ado para "enchufar y usar" cualquier sistema RAG externo que cumpla con una interfaz definida.
-   **GeneraciÃ³n Automatizada de Datasets:** Utiliza Large Language Models (LLMs) avanzados (ej., Llama 3) para generar automÃ¡ticamente preguntas y respuestas (ground truths) a partir de los documentos del RAG a evaluar.
-   **MÃ©tricas de Calidad RAG (Ragas):** Integra la librerÃ­a Ragas para calcular mÃ©tricas clave como:
    -   **Faithfulness (Fidelidad):** Â¿La respuesta del RAG estÃ¡ respaldada por el contexto?
    -   **Answer Relevancy (Relevancia de la Respuesta):** Â¿La respuesta es pertinente a la pregunta?
    -   **Context Precision (PrecisiÃ³n del Contexto):** Â¿El contexto recuperado es relevante?
    -   **Context Recall (Exhaustividad del Contexto):** Â¿Se recuperÃ³ todo el contexto necesario?
-   **MÃ©tricas Operacionales:** Incluye latencia y nÃºmero de tokens generados.
-   **Dashboard Interactivo:** Genera un informe HTML completo con visualizaciones interactivas (medias, distribuciones, correlaciones) para un anÃ¡lisis profundo de los resultados.
-   **IntegraciÃ³n de DeepEval (en el RAG Externo):** Fomenta la creaciÃ³n de pruebas unitarias y de regresiÃ³n en el propio RAG externo para una verificaciÃ³n de calidad continua.

## ğŸš€ CÃ³mo Empezar

### 1. Requisitos

-   Python 3.10+
-   `pip`
-   Acceso a una GPU (recomendado) para los LLMs open-source.
-   Un token vÃ¡lido de Hugging Face (requerido para modelos como Llama 3 y Gemma).
-   Acceso autorizado a los modelos restringidos de Hugging Face (ej., `meta-llama/Meta-Llama-3-8B-Instruct`, `google/gemma-2-2b-it`).
-   Opcional: Clave de API de OpenAI o Google si se prefiere para la generaciÃ³n de datasets o el juez de Ragas.

### 2. Estructura del Proyecto

rag_sandbox_evaluation/
â”œâ”€â”€ main.py # Punto de entrada principal para la evaluaciÃ³n
â”œâ”€â”€ requirements.txt # Dependencias del framework de evaluaciÃ³n
â”œâ”€â”€ .env # Variables de entorno (HF_TOKEN, ELASTIC_API_KEY, etc.)
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ components.py # Carga de LLMs y Embeddings (juez Ragas)
â”‚ â”œâ”€â”€ rag_interface.py # DefiniciÃ³n de la interfaz RAGSystem y RAGResult
â”‚ â”œâ”€â”€ my_custom_rag.py # Adaptador para el RAG externo a evaluar
â”‚ â”œâ”€â”€ evaluation.py # LÃ³gica de ejecuciÃ³n de Ragas
â”‚ â”œâ”€â”€ dashboard_rag.py # GeneraciÃ³n del dashboard HTML
â”‚ â””â”€â”€ dataset_generator.py # GeneraciÃ³n/carga del dataset de evaluaciÃ³n
â”œâ”€â”€ output/ # Salidas del dashboard y dataset
â””â”€â”€ .venv/ # Entorno virtual de Python

### 3. ConfiguraciÃ³n

#### a. Configurar tu RAG Externo

AsegÃºrate de que tu sistema RAG externo (ubicado en `/home/master/workspace/rag_sandbox/`):
-   EstÃ¡ estructurado como un paquete Python (ej. `rag_system_core`).
-   Tiene un `setup.py` que lista todas sus dependencias.
-   Define un punto de entrada (ej. `rag_system_core/rag_entrypoint.py`) que expone `initialize_application()`, `rag_core_instance`, `llm_display_name`, `active_pdf_filters` a nivel de paquete (vÃ­a `__init__.py`).
-   Su `rag_core.py` devuelve un diccionario `RAGResult` desde `process_query`.
-   Su `.env` contiene las configuraciones especÃ­ficas (rutas de PDFs, ChromaDB/Elasticsearch, LLM) para que funcione de forma independiente.

#### b. Configurar Variables de Entorno (`.env`)

En la raÃ­z de este proyecto (`rag_sandbox_evaluation/.env`), crea un archivo `.env` con tus credenciales:

HF_TOKEN="hf_YOUR_HUGGINGFACE_TOKEN_HERE"

Opcional, si usas OpenAI para generar el dataset

OPENAI_API_KEY="sk-YOUR_OPENAI_API_KEY_HERE"

Opcional, si has movido tus modelos de HF a una carpeta especÃ­fica

HF_HOME="/path/to/your/huggingface_models/"

#### c. Configurar `src/config.py`

Revisa `src/config.py` para ajustar los nombres de los LLMs (para el juez Ragas y la generaciÃ³n de dataset) y otras rutas de salida.

### 4. InstalaciÃ³n

1.  **Navega a la raÃ­z de este proyecto:**
    ```bash
    cd /home/master/workspace/rag_sandbox_evaluation
    ```
2.  **Crea y activa un entorno virtual:**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    pip install --upgrade pip setuptools wheel
    ```
3.  **Instala las dependencias, incluyendo el RAG externo como paquete editable:**
    ```bash
    pip install -r requirements.txt
    ```
    (AsegÃºrate de que tu `requirements.txt` contiene la lÃ­nea `-e /home/master/workspace/rag_sandbox` que apunta a la raÃ­z de tu RAG externo).

### 5. EjecuciÃ³n

1.  **Limpia los cachÃ©s (esencial despuÃ©s de cambios):**
    ```bash
    rm -rf __pycache__ src/__pycache__
    rm -rf /home/master/workspace/rag_sandbox/__pycache__ /home/master/workspace/rag_sandbox/rag_system_core/__pycache__ /home/master/workspace/rag_sandbox/rag_system_core.egg-info
    ```
2.  **Activa el entorno virtual** (si no lo estÃ¡).
3.  **Ejecuta el script principal de evaluaciÃ³n:**
    ```bash
    python main.py
    ```

## ğŸ“ˆ Resultados

El dashboard interactivo `rag_advanced_dashboard.html` se generarÃ¡ en la carpeta `output/` de este proyecto. Ãbrelo en tu navegador web para visualizar las mÃ©tricas y los anÃ¡lisis.
